%% abtex2-modelo-include-comandos.tex, v-1.9.5 laurocesar
%% Copyright 2012-2015 by abnTeX2 group at http://www.abntex.net.br/ 
%%
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License, either version 1.3
%% of this license or (at your option) any later version.
%% The latest version of this license is in
%%   http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 2005/12/01 or later.
%%
%% This work has the LPPL maintenance status `maintained'.
%% 
%% The Current Maintainer of this work is the abnTeX2 team, led
%% by Lauro César Araujo. Further information are available on 
%% http://www.abntex.net.br/
%%
%% This work consists of the files abntex2-modelo-include-comandos.tex
%% and abntex2-modelo-img-marca.pdf
%%

% ---
% Este capítulo, utilizado por diferentes exemplos do abnTeX2, ilustra o uso de
% comandos do abnTeX2 e de LaTeX.
% ---

%\chapter{Resultados de comandos}\label{cap_exemplos}
\chapter{About the Problem}\label{aboutTheProblem}

\iffalse
\chapterprecis{The purpose of this section if to motivate the problem.}\index{sinopse de capítulo}
\fi

\section{Search Space and Search Tree Formulation}
\cite{lelis2013predicting} defines the search space and search tree in the following way: Let the \textit{underlying search tree} (\textit{UST}) be the full brute$-$force tree created from a connected, undirected and implicitely defined \textit{underlying search graph} (\textit{USG}) describing a state space. Some search algorithms expand a subtree of the \textit{UST} while searching for a solution (\textsf{e.g.,} a portion of the \textit{UST} might not be expanded due to heuristic guidance); we call this subtree the \textit{expanded search tree} \textit{(EST)}.\\

In this dissertation our methods require to estimate the size of the subgraph expanded by an algorithm searching the \textit{USG}; and the subgraph expanded recieve the name of expanded search graph \textit{ESG}.\\

Let $G = (N,E)$ be a graph representing an \textit{ESG} where $N$ is its set of states and for each $n \in N\  op(n) = {op_{i}|(n, n_i) \in E}$ is its set of operators. The term edges and operators are used interchangeably and work as a successor function which recieves as input a state and action and returns another state.\\

Many search algorithms, such as IDA$\sp{*}$ \cite{Korf85ida}, expand a tree while searching the state space. By contrast, A$\sp{*}$ using a consistent heuristic expands a graph while searching the state space. The distinction between trees and graphs is important: Multiple nodes in a search tree might represent the same state in a search graph. In consequence, the tree might be substantially larger than the graph.\\

The purpose of using a search algorithm is to solve the problem or find the solution. We are insterested in finding a sequence of actions that goes from the start state to the goal state. To solve the problem is required the use of search algorithms. Two well know search algorithms are Depth First Search (\texttt{DFS}) and Breadth First Search (\texttt{BFS}). \texttt{DFS} looks for the solution traversing the search space exploring the nodes in each branch before backtracking up to find the solution. \texttt{BFS} looks for the solution exploring the neighbors nodes first, before moving to the next level of neighbors. Both search algorithms have the characteristic that generate a larger search space during the search. The search space that these algorithms generate are called Brute force search tree (\texttt{BFST}).\\

There are other type of algorithms called heuristic search algorithms, which are algorithms that requires the use of heuristics. The heuristic is the estimation of the distance for one node in the search tree to get the goal state. The heuristic search algorithms generate smaller search tree in comparison to the \texttt{BFST}, because the heuristic guides the search to more promising parts of the state space. Also, by reducing the search tree size, the heuristic function guidance might also reduce the overall running time of the algorithm.

There are different approaches to create heuristics, such as: Pattern Databases (\texttt{PDBs}) \cite{haslum2007domain}, Neural Network, and Genetic Algorithm \cite{edelkamp2007automated}. We call these systems Heuristic Generators. And one of the approaches that have showed most successfull results in heuristic generation is the \texttt{PDBs}, which are memory$-$based heuristic functions obtained by abstracting away certain problem variables, so that the remaining problem ``patten‘’ is small enough to be solved optimally for every state by blind exhaustive search. The results of such a blind search are stored in a table which represents a heuristic function for the original problem.\\

There exists many ways to take advantage of a large set of heuristic functions. For example: \cite{holte2006maximizing} showed that search can be faster if several smaller \texttt{PDBs} are used instead of one large pattern database. In addition \cite{domshlak2010max} and \cite{tolpin2013towards} showed that evaluating the heuristic lazily, only when they are essencial to a decision to be made in the search process is worthy in comparison to take the maximum of the set of heuristics. Then, using all the heuristics do not guarantees to solve the major number of problems in a limit time.
% ---
\section{Aim and Objectives}
\subsection{Aim}
\noindent
The objective of this dissertation is to develop meta-reasoning approaches for selecting heuristics functions from a large set of heuristics with the goal of reducing the running time of the search algorithms employing these functions.

\subsection{Objectives}
\noindent

\begin{itemize}
  \item Demostrate that the problem of finding the optimal subset of heuristics $\zeta\sp{*}$ of size $N$ for a given problem task is supermodular respect the size of the search tree.
  
  \item Develop an approach to find a subset of heuristics from a large pool of heuristics $\zeta$ that optimize the number of nodes expanded in the process of search.
  
  \item Develop an approach for selecting a subset of heuristics functions based on the size of the search tree and running time.

  \item Compare \texttt{SS} algorithm for predicting the search tree size of Iterative-Deepening A* (IDA*).
  
  \item Use \texttt{SS} as our utility function.

\end{itemize}
% ---
\section{Scope, Limitations, and Delimitations}
\noindent
We implemented our method in Fast Downward \cite{helmert2006fast} and we test our methods on the 2011 International Planning Competition (\texttt{IPC}) domain instances.\\

\section{Justification}
\noindent
In the last few decades, Artificial Intelligence has made significant strides in domain-independent planning. The use of heuristic search approach have contribuited to problem solving, where the use of an appropriate heuristic often means substantial reduction in the time needed to solve hard problems.\\

We use heuristic generators in order to create a large set of heuristics and obtain the most promosing heuristics to solve problems.\\

We believe that our idea of selecting heuristics using the size of the search tree and the running time will help us to solve more problems.

\section{Hypothesis}
\noindent
We test the following hypothesis:
\begin{itemize}
\item \textbf{H1:} Test that the greedy algorithms are effective for selecting a subset of heuristics to guide search.

\item \textbf{H2:} Test that \texttt{SS} is an effective prediction method to our meta-reasoning.
\end{itemize}

\section{Contribution of the Dissertation}
\noindent
The main contributions of this Dissertation are:
\begin{itemize}
\item Provide a meta-reasoning approach for selecting heuristic functions while minimizing the number of nodes expanded by the selecting heuristics.

\item Provide a meta-reasoning approach for selecting heuristic functions while minimizing the running time of the search. 
\end{itemize}

\section{Organization of the Dissertation}
\noindent
The Dissertation is organized as follows: 
\begin{enumerate}
\item In Chapter I, the background of the dissertation is provided. Which also includes our motivation and the scope definition.
\item In Chapter II, we review the state of the art in selection of heuristic functions.
\item In Chapter III, we introduce Random Greedy Heuristic Selection (\texttt{RGHS}) and the prediction methods. 
\item In Chapter IV, we explain the results obtained by using \texttt{RGHS} and compare it with other planner systems.
\item We conclude in Chapter V.
\end{enumerate}

In the next chapter, the domain 8$-$tile$-$puzzle is used to understand the concepts that will be helpful for the other chapters. \\

\clearpage