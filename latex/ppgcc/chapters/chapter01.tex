%% abtex2-modelo-include-comandos.tex, v-1.9.5 laurocesar
%% Copyright 2012-2015 by abnTeX2 group at http://www.abntex.net.br/ 
%%
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License, either version 1.3
%% of this license or (at your option) any later version.
%% The latest version of this license is in
%%   http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 2005/12/01 or later.
%%
%% This work has the LPPL maintenance status `maintained'.
%% 
%% The Current Maintainer of this work is the abnTeX2 team, led
%% by Lauro César Araujo. Further information are available on 
%% http://www.abntex.net.br/
%%
%% This work consists of the files abntex2-modelo-include-comandos.tex
%% and abntex2-modelo-img-marca.pdf
%%

% ---
% Este capítulo, utilizado por diferentes exemplos do abnTeX2, ilustra o uso de
% comandos do abnTeX2 e de LaTeX.
% ---

%\chapter{Resultados de comandos}\label{cap_exemplos}
\chapter{About the Problem}\label{aboutTheProblem}

\iffalse
\chapterprecis{The purpose of this section if to motivate the problem.}\index{sinopse de capítulo}
\fi
% ---
\section{Problem Statement and Motivation}
% ---

\noindent
Every problem of Artificial Intelligent can be cast as a state space problem. The state space is a set of states where each state represent a possible solution to the problem and each state is linked with other states if exists a function that goes from one state to another. In the search space there are many solutions that represent the same state, each of this solutions are called node. So, many nodes can be represented as one state. To find the solution of the problem is required the use of search algorithms such as: Depth First Search (\texttt{DFS}), which looks the solution of the problem traversing the search space exploring the nodes in each branch before backtracking up to find the solution. Another search algorithm is Breadth First Search (\texttt{BFS}), which looks for the solution exploring the neighbors nodes first, before moving to the next level of neighbors. The mentioned algorithms have the characteristic that when they do the search, they generate a larger search space. The search space that these algorithms generate are called Brute force search tree (\texttt{BFST}). \\

There are other types of algorithms called heuristics informed search, which are algorithms that requires the use of heuristics. The heuristic is the estimation of the distance for one node in the search tree to get to the near solution. The heuristic informed search generates smaller search tree in comparison to the \texttt{BFST}, because the heuristic guides the search exploring the nodes that are in the solution path and prunes the nodes which are not. Also, the use of heuristics reduce the running time of the search algorithm. \\

There are different approaches to create heuristics, such as: Pattern Databases (\texttt{PDBs}), Neural Network, and Genetic Algorithm. These systems that create heuristics receive the name of Heuristics Generators. And one of the approaches that have showed most successfull results in heuristic generation is the PDBs, which is memory-based heuristic functions obtained by abstracting away certain problem variables, so that the remaining problem ("pattern") is small enough to be solved optimally for every state by blind exhaustive search. The results stored in a table, represent a PDB for the original problem. The abstraction of the search space gives an admissible heuristic function, mapping states to lower bounds. \\

Exists many ways to take advantage of all the heuristics that can be created, for example: \cite{holte2006maximizing} showed that search can be faster if several smaller pattern databases are used instead of one large pattern database. In addition \cite{domshlak2010max} and \cite{tolpin2013towards} results showed that evaluating the heuristic lazily, only when they are essensial to a decision to be made in the search process is worthy in comparison to take the maximum of the set of heuristics. Then, using all the heuristics do not guarantees to solve the major number of problems in a limit time.
% ---
\section{Aim and Objectives}
\subsection{Aim}
\noindent
The objective of this dissertation is to develop meta-reasoning approaches for selecting heuristics functions from a large set of heuristics with the goal of reducing the running time of the search algorithm employing these functions.

\subsection{Objectives}
\noindent

\begin{itemize}
  \item Demostrate that the problem of finding the optimal subset of $\zeta$ of size $N$ for a given problem task is supermodular respect the size of the search tree.

  \item Develop an approaches to obtain the cardinality of the subsets of heuristics found.
  
  \item Develop an approach to find a subset of heuristics from a large pool of heuristics that optimize the number of nodes expanded in the process of search.
  
  \item Develop an approach for selecting a subset of heuristics functions based on the evaluation cost of each heuristic.
  
  \item Develop an strategy to drop heuristics during the sampling that do not improve the objective function.  
  
  \item Use Stratified Sampling (SS) algorithm for predicting the search tree size of Iterative-Deepening A* (IDA*). And use SS as our utility function.
   
\end{itemize}
% ---
\section{Scope, Limitations, and Delimitations}
\noindent
We implemented our method in Fast Downward \cite{helmert2006fast} and the problems we want to solve are the 2011 International Planning Competition (\texttt{IPC}) domain instances.\\

We used the 2011 instances instead of the 2014 instances because the former do not have problems with conditional effects, which are currently not handled by \texttt{PDB} heuristics.\\

Our meta$-$reasoning described in this thesis are going to try to solve the major number of problems using the most promising heuristics from a large set of heuristics. The exact way to create the large set of heuristics is beyond the scope of this thesis.

\section{Justification}
\noindent
In the last few decades, Artificial Intelligence has made significant strides in domain$-$independent planning. The use of heuristics search approach have contribuited to problem solving, where the use of an appropriate heuristic often means substantial reduction in the time needed to solve hard problems.\\

That is why we propose a meta$-$reasoning that will try to solve the major number of problems without relying on domain knowledge, to guide the A$\sp{*}$ search algorithm.

\section{Hypothesis}
\noindent
This thesis will intend to prove the hypotheses listed below:
\begin{itemize}
\item \textbf{H1:} Prove that our objective function of selection is supermodular respect to the A$\sp{*}$ search tree size and respect to the A$\sp{*}$ running time.

\item \textbf{H2:} Prove that \texttt{SS} is an effective prediction method to our meta$-$reasoning.
\end{itemize}

\section{Contribution of the Thesis}
\noindent
The main contributions of this Thesis are:
\begin{itemize}
\item Provide a meta$-$reasoning approach based on the size of the search tree generated.
\item Provide a meta$-$reasoning approach based on the evaluation cost of each heuristic. 
\end{itemize}

\section{Organization of the Thesis}
\noindent
The Thesis is organized as follows: 
\begin{enumerate}
\item In Part I, the background of the thesis is provided which also includes our motivation and define the scope. 
\item In Part II, we review the state of the art in selection.
\item In Part III, we introduce Random Greedy Heuristic Selection (\texttt{RGHS}) and the prediction methods. 
\item In Part IV, we explain the results obtained by using \texttt{GRHS} and compared with other planner systems. 
\item We conclude in Part V, by discussing the concluding remarks.
\end{enumerate}

In the next chapter, the domain 8$-$tile$-$puzzle is used to understand the concepts that will be helpful for the other Parts. \\

\clearpage