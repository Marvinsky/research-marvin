
\documentclass[11pt,a4paper,oneside]{report}

\synctex=1

\usepackage{pslatex,palatino,avant,graphicx,color}
\usepackage[margin=2cm]{geometry}

\usepackage{mathptmx}       % selects Times Roman as basic font

\usepackage{helvet}         % selects Helvetica as sans-serif font
\usepackage{courier}        % selects Courier as typewriter font
\usepackage{type1cm}        % activate if the above 3 fonts are
                            % not available on your system

\usepackage{makeidx}         % allows index generation
\usepackage{graphicx}        % standard LaTeX graphics tool
                             % when including figure files
\usepackage{multicol}        % used for the two-column index
\usepackage{multirow}
\usepackage{booktabs}

\usepackage[bottom]{footmisc}% places footnotes at page bottom

\usepackage[utf8]{inputenc}
\inputencoding{utf8}

\usepackage[square,sort]{natbib}
\bibliographystyle{unsrtnat}


\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{longtable}
\usepackage{pdflscape}

%for formula references
\usepackage{amsmath}

\newtheorem{mydef}{Definition}

\usepackage{changes}
%triangledown
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}

\usepackage[]{algorithm2e}

%langle
\usepackage{scalerel}
\usepackage{graphicx}


\makeindex

\begin{document}


\begin{titlepage}

\begin{center}
\vspace*{-1in}
\begin{figure}[htb]
\begin{center}
\includegraphics[width=8cm]{./image/ufv1}
\end{center}
\end{figure}

CENTRO DE CIENCIAS EXATAS E TECNOLOGICAS - CCE\\
\vspace*{0.15in}
DEPARTAMENTO DE INFORMATICA \\
\vspace*{0.6in}
\begin{large}
PROJECT:\\
\end{large}
\vspace*{0.2in}
\begin{Large}
\textbf{ON SELECTING HEURISTIC FUNCTIONS FOR DOMAIN-INDEPENDENT PLANNING.} \\
\end{Large}
\vspace*{0.3in}
\begin{large}
%A Thesis Project submitted by Marvin Abisrror for the degree of Master to the PPG\\
Marvin Abisrror\\
\end{large}
\vspace*{0.3in}
\rule{80mm}{0.1mm}\\
\vspace*{0.1in}
\begin{large}
Supervised by: \\
Levi Henrique Santana de Lelis, Santiago Franco \\
\end{large}
\end{center}

\end{titlepage}


\tableofcontents
\newpage

\section{Introduction}
Solving independent domains is a very hard task when we do planning. Sometimes we will find instances that takes too much time to find the solution, and the measure of that time could be hours, days or even years. Users of search algorithms usually do not know \textit{a priori} how long it will take to solve a problem instance $-$ some instances could be solved quickly while others could take long time. Many researchs in the area of heuristic search explains why this could be happening, some of those reasons are: That the size of the tree is very large, the inconsistency of the heuristic used to do the search or the complexity of the instance that could contains cycles or even dead end areas which avoid to make any movement in the search space tree, etc.\\

Furthermore, domain independent planning is related to the general problems solving where we basically require a very inteligent domain-independent search algorithm or heuristic to do the search in order to solve the instance without knowing in detail how is the problem and the search space it produces.\\

Also, the need to solve the major number of problems using restricted resources of memory and time is vital in the area of Artificial Intelligence (AI), because each time are released problems with high grade of difficulty that can not be solved using the last state of the art in planning. In addition, the seek for solving domain independent planning is critical is the areas manufacturing, space systems, software engineers, robotics, education, and entertainment.\\

On the whole, finding an strategy which allow us to solve the major number of problems using domain indepenent planning is a very important task in AI.\\

Finally, the proposal of this project is to develop an approach of selecting heuristics functions to optimize the search of the solution applied by the domain independent planning. In order to reduze the amount of memory and time.



\section{Problem and Importance}
This project is concerned with cost-optimal state-space planning using the A* algorithm \citep{hart1968formal}. We asume that a pool, $\zeta$, of hundreds or even thousands of heuristics is available, and that the final heuristic used to guide A*, $h_{\substack{max}}$, will be defined as the maximum over a subset $\zeta\sp{\prime}$ of those heuristics ($h_{\substack{max}(s, \zeta\sp{\prime})} = \max\limits_{ h\epsilon \zeta\sp{\prime}}h(s)$). The choice of the subset $\zeta\sp{\prime}$ can greatly affect the efficiency of A*. For a given size \textit{N} and planning task $\triangledown$, a subset containing \textit{N} heuristics from $\zeta$ is optimal if no other subset containing \textit{N} heuristics from $\zeta$ results in A* expanding fewer nodes when solving $\triangledown$.\\

\section{Hypotheses}
We present a greedy method based on the theory of supermodular optimization for selecting a subset of heuristic functions, without relying on domain knowledge, to guide the A* search algorithm. If the heuristics are consistent, our method selects a subset which is guaranteed to be near optimal to the resulting A* search tree size. Furthermore to being consistent, if all heuristics have the same evaluation time, we expect the subset would be near-optimal with respect to the resulting A* search tree size.\\

\section{Objectives}
The first contribution of this research is to show that the problem of finding the optimal subset of $\zeta$ of size \textit{N} for a given problem task is supermodular, and therefore a greedy algorithm, which we call Greedy Heuristic Selection (GHS), that selects heuristics from $\zeta$ one at a time is guaranteed to produce a subset $\zeta\sp{\prime}$ such that the number of nodes expanded by A* is no more than approximately 1.36 times optimal. Moreover, if all heuristics in $\zeta$ have the same evaluation time, then the A* running time using $\zeta\sp{\prime}$ is also guaranteed to be no more than approximately 1.36 times optimal. An optimization procedure which is similar to ours is presented by \citep{raynersss13}, but their procedure maximizes the average heuristic value. By contrast, GHS minimizes the search tree size.\\

GHS requires a prediction of the number of nodes expanded by A* using any given subset. Although there are methods for accurately predicting the number of nodes expanded by Iterative Deepening-A* \citep{korf2001timecomplexity} (IDA*), Stratified Sampling \citep{lelis2013predicting} (SS), these methods can't be easily adapted to A* because A*'s duplicate pruning makes it very difficult to predict how many nodes will occur at depth \textit{d} of A*'s search tree (the tree of nodes expanded by A*). In this research we want to make another contribution showing that the Stratification Sampling(SS) generates very good predictions for the A*'s search tree.\\

The second contribution of this research is an empirical evaluation of GHS in optimal domain-independent planning problems. Namely, we implemented and evaluated GHS on the planning problems from the 2011 International Planning Competition (IPC).\\

The main aim of this research is to develop an approach to resolves problems quickly. The approach we are proposing can be applied for solving optimal domain problems in the Fast-Downward Planner \citep{helmert2006fast}. A few of these domains are: Elevators opt08 and opt11, Floortile opt11, Nomystery opt11, etc.
\\

\section{Literature}
State-space search algorithms, such as A* \citep{hart1968formal}, are fundamental in many AI applications. A* uses the $f(s) = g(s) + h(s)$ cost function to guide its search. Here, $g(s)$ is the cost of the path from the start state to state \textit{s}, and $h(s)$ is the estimated cost-to-go from \textit{s} to a goal; $h(.)$ is known as the heuristic function. A heuristic \textit{h} is \textit{admissible} if $h(s) \leq dist(s, goal)$ for every state \textbf{s} and goal state \textit{goal}, where \textit{dist(n, m)} is the cost of at least-cost path from \textit{n} to \textit{m}. If $h(n)$ is admissible, \textit{i,e.} always returns a lower bound estimate of the optimal cost, these algorithms are guaranteed to find an optimal path from the start state to a goal state if one exists.\\

In addition to being admissible the heuristic also could be \textit{consistent}. A heuristic \textit{h} is \textit{consistent} if for every pair of states, \textit{m} and \textit{n}, $h(m) - h(n) \leq dist(m, n)$


\section{Metodology}
sdfsdfsdf

\section{Timetable}
ssfdsf






\section{Problem Formulation}
When solving $\triangledown$ using the consistent function $h_{\substack{max}}(\zeta\sp{\prime})$, for $\zeta\sp{\prime} \subseteq \zeta$, A* expands in the worst case $J(\zeta\sp{\prime}, \triangledown)$ nodes, where:

\begin{equation}
\label{eq:1}
J(\zeta\sp{\prime}, \triangledown)  = |\{ s \epsilon V | f_{\substack{max}}(s, \zeta\sp{\prime}) \leq C\sp{\star}  \}|
\end{equation}

\begin{equation}
\label{eq:2}
J(\zeta\sp{\prime}, \triangledown)  = |\{ s \epsilon V | h_{\substack{max}}(s, \zeta\sp{\prime}) \leq C\sp{\star} - g(s)\}|
\end{equation}

We write $J(\zeta\sp{\prime})$ instead of $J(\zeta\sp{\prime}, \triangledown)$ whenever $\triangledown$ is clear from context.\\

We present a greedy algorithm for approximately solving the following optimization problem.

\begin{equation}
\label{eq:3}
	\min\limits_{\zeta\sp{\prime} \epsilon2\sp{|\zeta|}  }    J(\zeta\sp{\prime}, \triangledown)
\end{equation}

\begin{equation}
\label{eq:4}
	subject\ to\ \ |\zeta\sp{\prime}| = N
\end{equation}

where \textit{N} could be determined by a hard constraint such as the maximum number of PDBs one can store in memory.

%Imports the bibliography file "references.bib"
\bibliography{references}
%\bibliographystyle{references}

\end{document}
