%% abtex2-modelo-include-comandos.tex, v-1.9.5 laurocesar
%% Copyright 2012-2015 by abnTeX2 group at http://www.abntex.net.br/ 
%%
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License, either version 1.3
%% of this license or (at your option) any later version.
%% The latest version of this license is in
%%   http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 2005/12/01 or later.
%%
%% This work has the LPPL maintenance status `maintained'.
%% 
%% The Current Maintainer of this work is the abnTeX2 team, led
%% by Lauro César Araujo. Further information are available on 
%% http://www.abntex.net.br/
%%
%% This work consists of the files abntex2-modelo-include-comandos.tex
%% and abntex2-modelo-img-marca.pdf
%%

% ---
% Este capítulo, utilizado por diferentes exemplos do abnTeX2, ilustra o uso de
% comandos do abnTeX2 e de LaTeX.
% ---

%\externaldocument[I-]{chapter03}

\chapter{Empirical Evaluation}\label{ch:empirical_evaluation}


\noindent
This chapter is organized as follows. First we show that \texttt{SS} is able to produce very accurate estimates of the number of nodes generated by IDA*, a search algorithm that does not detect duplicated nodes during search, while solving domain-independent planning problems. Next, we show that \texttt{SS} produces very poor estimates of the number of nodes generated by A* on the same problem domains. Although \texttt{SS} is unable to produce good predictions for A*, we show empirically that the \texttt{SS}'s predictions allow one to select, from a pool of heuristics, the one that will result in the smallest A* search tree size. 

After testing the effectiveness of \texttt{SS}, we test whether the heuristic subsets selected by \texttt{GHS} (using both \texttt{SS} and \texttt{CS}) are near optimal. That is, we are interested in knowing how $J(\zeta', \nabla)$ compares with $J(\zeta, \nabla)$, which is provably optimal. 

Finally, we compare the effectiveness of \texttt{GHS} with other state-of-the-art planners. 

\section{SS for Predicting the IDA* Search Tree Size}

\noindent
\texttt{SS} was shown to produce good predictions of the IDA$\sp{*}$ search tree size (Lelis et al., \citeyear{lelis2013predicting}). IDA$\sp{*}$ is a heuristic search algorithm that uses the same cost function used by A* but it searches the state space in a depth-first manner. Moreover, in contrast with A*, IDA* does not detect duplicated nodes.

%for looking the solution in the search space and in each iteration uses \texttt{DFS} to prune the branch until reaching the cost threshold (Korf, Reid and Edelkamp, \citeyear{korf2001timecomplexity}).
 
In this section we show that \texttt{SS} is able to produce good predictions for IDA$\sp{*}$ also in domain-independent planning, and we will show that it produces very inaccurate predictions for A$\sp{*}$. In this experiment \texttt{SS} estimates the search tree size generated by IDA$\sp{*}$ using a consistent heuristic. \texttt{SS} estimates the size of the search tree up to some defined $f$-layer in the tree.

We first run IDA$\sp{*}$ in domain-independent planning benchmark problems with a 30-minute time limit. Then we run \texttt{SS} limited by different $f$-layers encountered in the IDA$\sp{*}$ searches. We experiment with the following number of probes: 1, 10, 100, 1000 and 5000.

We use the \textit{relative unsigned error}~\cite{lelis2012fast}, which we call relative-error, for short, to measure prediction accuracy. The relative-error is shown in Equation \ref{eq:eq_comparison} below.

\begin{equation}
\frac{\sum_{s\in PI} \frac{Pred(s, d) - R(s, d)}{R(s, d)}}{|PI|}
\label{eq:eq_comparison}
\end{equation}

\begin{itemize}
  \item $PI$ is the set of all instances of a problem domain.
  \item $Pred(s,d)$ is the estimated search tree size generated by IDA$\sp{*}$ for start state $s$ and cost bound $d$.
  \item $R(s,d)$ is the real number of nodes expanded by IDA$\sp{*}$ for start state $s$ and cost bound $d$.
\end{itemize}
A relative error of 0.0 means that \texttt{SS} was able to produce perfect predictions.

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[]
%\footnotesize\setlength{\tabcolsep}{1.2pt}
\tiny\setlength{\tabcolsep}{2pt}
\centering
\caption{Comparison between \texttt{SS} and IDA$\sp{*}$ for 1, 10, 100, 1000 and 5000 probes using $hmax$ heuristic.}
\label{tb:comparison}
\begin{tabular}{lrrrrrrrrrrrrr}
\hline
\multirow{3}{*}{Domain} & \multicolumn{13}{c}{$hmax$}                                                                                                                     \\ \cline{2-14} 
                        & \multirow{2}{*}{IDA$\sp{*}$} & \multirow{2}{*}{time} & \multicolumn{5}{c}{relative$-$error} & \multicolumn{5}{c}{time}   & \multirow{2}{*}{n} \\ \cline{4-13}
                        &                              &                       & 1   & 10   & 100   & 1000   & 5000   & 1 & 10 & 100 & 1000 & 5000 &                    \\ \hline
Barman & 8835990.00& 6016.38& 0.60& 0.45& 0.20& 0.07& 0.04& 0.06& 0.32& 3.21& 32.57& 214.59& 20\\
Elevators & 1012570.00& 4987.57& 0.84& 0.42& 0.23& 0.13& 0.10& 1.40& 9.85& 96.37& 994.33& 4425.93& 20\\
Floortile & 30522300.00& 3919.72& 2.02& 0.62& 0.40& 0.14& 0.11& 0.01& 0.07& 0.69& 6.93& 36.60& 2\\
Nomystery & 6565740.00& 3256.86& 0.53& 0.26& 0.07& 0.03& 0.01& 0.07& 0.38& 3.63& 36.35& 181.03& 20\\
Openstacks & 80108.50& 4017.19& 0.03& 0.03& 0.03& 0.03& 0.03& 94.79& 774.86& 1067.84& 10929.00& 11174.30& 20\\
Parcprinter & 1.00& 0.00& 0.00& 0.00& 0.00& 0.00& 0.00& 0.01& 0.04& 0.35& 3.48& 17.29& 20\\
Parking & 374925.00& 5607.50& 0.17& 0.04& 0.01& 0.00& 0.00& 1.79& 11.36& 114.28& 1196.83& 5835.03& 20\\
Pegsol & 68763.70& 5.00& 0.17& 0.04& 0.02& 0.01& 0.00& 0.01& 0.04& 0.37& 3.69& 17.88& 20\\
Scanalyzer & 8449890.00& 4920.58& 0.43& 0.25& 18.63& 0.02& 0.01& 3.13& 28.79& 273.74& 3033.06& 10254.00& 20\\
Sokoban & 3118530.00& 3932.69& 0.41& 0.26& 0.11& 0.05& 0.04& 0.31& 2.00& 21.42& 222.47& 1056.61& 20\\
Tidybot & 444473.00& 5632.08& 300.86& 1072.40& 5.88& 0.01& 0.01& 4.40& 26.48& 238.76& 2747.10& 11925.40& 20\\
Transport & 2622880.00& 2253.51& 0.63& 0.54& 0.24& 0.15& 0.11& 0.09& 0.61& 5.89& 59.37& 290.31& 20\\
Visitall & 71032400.00& 3704.78& 0.12& 0.04& 0.01& 0.00& 0.00& 0.00& 0.05& 0.56& 5.77& 28.07& 20\\
Woodworking & 5139070.00& 4944.76& 1.28& 0.69& 0.27& 0.17& 0.07& 0.15& 1.33& 13.21& 130.82& 664.08& 20\\ \hline
\end{tabular}
\end{table}

The Table \ref{tb:comparison} shows the relative-error for different number of probes. The heuristic used in this experiment is $hmax$. The column $n$ shows the number of problems that were used to compute the relative-error, the average number of nodes generated by IDA$\sp{*}$, and the average running time of IDA* and \texttt{SS}. The table of results also shows the average number of nodes generated by IDA$\sp{*}$. % and IDA$\sp{*}$'s average running time. 

The relative-error tends to reduce as one increases the number of probes. For example, in Barman, the relative-error goes from $0.60$ for 1 probe to $0.45$ for 10 probes, $0.20$ for 100 probes, $0.07$ for 1000 probes, and $0.04$ for 5000 probes. In the case of running time, while the number of probes increase, \texttt{SS} needs to spend more time approximating the size of the search tree. That is why the overall running time of \texttt{SS} increases as one increases the number of probes. For example, in Barman, the time goes from $0.06$ seconds for 1 probe to $0.32$ seconds for 10 probes, $3.21$ seconds for 100 probes, $32.57$ seconds for 1000 probes and $214.59$ seconds for 5000 probes. There are domains such as: Parcprinter, Parking, Pegsol and Visitall that have perfect score using 5000 probes. In the case of Tidybot, the relative-error using 1 probe is smaller than using 10 probes, which happens due to the stochastic nature of \texttt{SS}.

Each domain of the 2011 \texttt{IPC} benchmark contains 20 instances. The results of Floortile in Table \ref{tb:comparison} were computed using only 2 instances ($n=2$). This is because IDA* was able to fully complete an iteration for its initial cost bound for only two instances (opt-p01-001 and opt-p03-006). In summary, we showed that for 2011 \texttt{IPC} domains, \texttt{SS} is able to produce good predictions of the search tree expanded by IDA$\sp{*}$. % when the number of probes goes to infinity.

\section{SS for Predicting the A* Search Tree Size}

\noindent
The Table \ref{tb:ipdb_lmcut_mands} shows the relative-error of \texttt{SS} while predicting the number of nodes generated by A*. In this experiment we tested different heuristic functions: iPDB \cite{haslum2007domain}, \texttt{LM-Cut} \cite{PommereningH13} and M$\&$S \cite{nissim2011computing}. The column $n$ shows the number of instances solved by A$\sp{*}$ using the three heuristics. For this experiment we only use the instances that are independently solved by A* using each of the three heuristics within a 30-minute time limit. The columns with A$\sp{*}$ represents the average of number of nodes expanded by A$\sp{*}$. The column with \texttt{SS}-error represents the relative-error for the solved instances. We used 500 probes in this experiment. 


%(iPDB, LM-Cut, and M\&S). The A* column shows the average% number of nodes generated by A*. The column $n$ shows the number of problem instances used in this experiment. We only considered problem instances that A* was able to solve within a 30-minute time limit. 

%that \texttt{SS} is not a good estimator for A$\sp{*}$. This is because \texttt{SS} does not account for duplicate nodes and A$\sp{*}$ does. As a result \texttt{SS} usually overestimates by a several orders of magnitude the A$\sp{*}$ search tree size.

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[!htb]
\footnotesize
%\setlength{\tabcolsep}{4.2pt}
\centering
\caption{Poor prediction of SS against A* using ipdb, \texttt{LM-Cut} and M$\&$S with 500 probes}
\label{tb:ipdb_lmcut_mands}
\begin{tabular}{lrrrrrrr}
\hline
\multirow{2}{*}{Domain} & \multicolumn{2}{c}{ipdb} & \multicolumn{2}{c}{\texttt{LM-Cut}} & \multicolumn{2}{c}{M$\&$S} & \multirow{2}{*}{n} \\ \cline{2-7}
                     & A*          & relative-error         & A*                & relative-error                & A*           & relative-error          &                    \\ \hline
Barman               & 1.72e+07    & 8.68e+31   & 7.45e+06          & 2.21e+30          & 6.67e+06     & 1.26e+36    & 4                  \\
Floortile            & 1.40e+07    & 1.74e+18   & 702435            & 4.68e+14          & 4.46e+06     & 1.90e+12    & 4                  \\
Nomystery            & 40169.7     & 6.71e+32   & 267100            & 6.14e+19          & 8236         & 1.20e+20    & 9                  \\
Openstacks           & 570099      & 0.61884    & 570099            & 0.677425          & 569984       & 0.672143    & 4                  \\
Parcprinter          & 1157        & 2.56e+22   & 1363.67           & 2.33e+21          & 766.333      & 6.36e+20    & 3                  \\
Pegsol               & 841693      & 2901.39    & 398221            & 6859.86           & 933430       & 779.017     & 16                 \\
Scanalyzer           & 337894      & 3.94e+33   & 334747            & 7.58e+31          & 337833       & 2.42e+31    & 3                  \\
Sokoban              & 376755      & 1.04e+07   & 45374             & 2.74e+06          & 739775       & 5.60e+08    & 9                  \\
Transport            & 1.89e+06    & 2.91e+38   & 1.49e+06          & 1.15e+25          & 1.73e+06     & 1.50e+29    & 2                  \\
Visitall             & 253710      & 1.69e+46   & 253195            & 1.69e+46          & 253521       & 1.71e+46    & 8                  \\
Woodworking          & 3.21e+06    & 2.53e+18   & 3.20e+06          & 2.76e+18          & 3.21e+06     & 2.48e+18    & 3                  \\ \hline
\end{tabular}
\end{table}


As can be observed in Table \ref{tb:ipdb_lmcut_mands}, \texttt{SS} usually overestimates the number of nodes expanded by A$\sp{*}$ by several orders of magnitude, for example, in Barman %with M$\&$S, A$\sp{*}$ expands on average $6.67\texttt{Time}s10\sp{6}$ nodes which is less nodes than iPDB$-1.72\texttt{Time}s10\sp{7}$ and \texttt{LM-Cut}$-7.45\texttt{Time}s10\sp{6}$. However,
\texttt{SS}'s relative-error is $1.26\times10\sp{36}$ (M$\&$S), $8.68\times10\sp{31}$ (iPDB), and $2.21\times10\sp{30}$ (\texttt{LM-Cut}). %which  that the number of nodes expanded by \texttt{SS} in average is in the order of magnitude from 30 to 40. 
%In Visitall, \texttt{SS}-error shows that \texttt{SS} overestimates A$\sp{*}$ highly, which represent a very bad prediction of \texttt{SS}. In Openstack: Using the three heuristics, A$\sp{*}$ expands almost the same number of nodes for the 4 instances solved, and \texttt{SS}-error shows a score near to the perfect and the reason is that \texttt{SS} expands less nodes than A$\sp{*}$. 
The exception in Table~\ref{tb:ipdb_lmcut_mands} is Openstacks, where \texttt{SS} produces accurate predictions. This is probably because the A* search tree does not have a large number of duplicated nodes in this domain. That is, the A* search tree is similar to the IDA* search tree for Openstack problems. 


\section{Approximation Analysis for SS and A*}

\noindent
Although \texttt{SS} is not able to produce good predictions of the A$\sp{*}$ search tree size, here we show empirically that \texttt{SS} allows one to make good heuristic subset selections. In this experiment we use the following heuristics: iPDB, \texttt{LM-Cut}, and 10 independently generated \texttt{GA-PDBs}~\cite{edelkamp2007automated}.

Our goal with this experiment is to show that in order to select heuristics, the predictions produced by \texttt{SS} do not have to be accurate in absolute terms, but they have to be accurate in relative terms. That is, if A$\sp{*}$ using heuristic $h_{1}$ expands fewer nodes than when using $h_{2}$, then \texttt{SS}'s predictions could be arbitrarily inaccurate for both $h_{1}$ and $h_{2}$, as long as the prediction estimates that A* expands fewer node when using $h_{1}$. % is better than $h_{2}$ then one is able to select $h_{1}$ over $h_{2}$. 

In order to investigate whether \texttt{SS} is able to allow one to select the best heuristic in a set of size two: $\{h_{1}, h_{2}\}$, we construct a graph where the y-axis is the ratio $J(h_{1})/J(h_{2})$ (the actual number of nodes generated by A$\sp{*}$ using $h_{1}$ to the number of nodes generated by A$\sp{*}$ using $h_{2}$); and the x-axis is the ratio $\hat{J}(h_{1})/\hat{J}(h_{2})$ (the predicted search tree size of $h_{1}$ to the predicted search tree size of $h_{2}$). 
%
%We limit our plot to the value of 2, and any fraction that is greater than 2 is set to 2. 
The plot is divided in four quadrants, as shown in Figure \ref{fig:img_cartesian_plane}. If a data point falls in quadrants II and III it means that \texttt{SS} is able to correctly select the heuristic that will 
yield the smallest search tree size. 
%make A* expand fewer number of nodes. 
If the point falls in quadrants I or IV it means that \texttt{SS} selects the heuristic that will yield the largest tree size. 

\pagestyle{empty}

\begin{figure}[!thb]
\centering  
\begin{tikzpicture}[scale=2]
  \draw[->] (-0.2,0) -- (2.1,0) node[right] {($\hat{J}(h_{1})$/$\hat{J}(h_{2})$)};
  \draw[->] (0,-0.2) -- (0,2.1) node[above] {($J(h_{1})$/$J(h_{2})$)};

  \draw[densely dotted] (-0.1,1) -- (2.1,1);
  \draw[densely dotted] (1,-0.1) -- (1,2.1);
  
  \draw[densely dotted] (-0.1,2) -- (2.1,2);
  \draw[densely dotted] (2,-0.1) -- (2,2.1);

  \draw[shift={(0.5,1.5)}] node[below] {I};
  \draw[shift={(1.5,1.5)}] node[below] {II};
  \draw[shift={(0.5,0.5)}] node[below] {III};
  \draw[shift={(1.5,0.5)}] node[below] {IV};


  \foreach \x/\xtext in {1/1, 2/2}
    \draw[shift={(\x,0)}] (0pt,2pt) -- (0pt,-2pt) node[below] {$\xtext$};

  \foreach \y/\ytext in {1/1, 2/2}
    \draw[shift={(0,\y)}] (2pt,0pt) -- (-2pt,0pt) node[left] {$\ytext$};

\end{tikzpicture}
  \caption{Cartesian Plane with domain $\left\langle 0, 2\right\rangle$ and range $\left\langle 0, 2\right\rangle$. Note that the ratio values could be larger than 2, and we assume that any ratio larger than 2 is automatically set to 2 in this experiment.}\label{fig:img_cartesian_plane}
\end{figure}

%Here is a summary of the relation of \texttt{SS} predictions and actual number of nodes generated by A* for 

A data point falls in each quadrant (I, II, III, and IV) if the corresponding boolean expression is true. 

\begin{enumerate}[label=\Roman*] 
\item - $J(h_{1}) > J(h_{2}) \wedge \hat{J}(h_{2}) \ge \hat{J}(h_{1})$ 
\item - $\big(J(h_{1}) > J(h_{2}) \wedge \hat{J}(h_{1}) > \hat{J}(h_{2})\big) \vee J(h_{1}) = J(h_{2})$.
\item - $J(h_{2}) > J(h_{1}) \wedge \hat{J}(h_{2}) > \hat{J}(h_{1})$.
\item - $J(h_{2}) > J(h_{1}) \wedge \hat{J}(h_{1}) \ge \hat{J}(h_{2})$.
\end{enumerate}

%We note that 
We assign a data point to quadrant II if $J(h_{1}) = J(h_{2})$ because in this situation A* generates the same number of nodes when using $h_1$ or $h_2$.

\begin{table}[htb]
\centering
\begin{tabular}{lc}
\hline
Domain      & II and III ($\%$) \\ \hline
Elevators   & 78.57    \\
Floortile   & 96.08    \\
Nomystery   & 71.82    \\
Parcprinter & 70.50    \\
Pegsol      & 96.83    \\
Scanalyzer  & 100.00   \\
Sokoban     & 89.31    \\
Tidybot     & 100.00   \\
Transport   & 51.78    \\
Visitall    & 98.05    \\
Woodworking & 100.00   \\ \hline
\end{tabular}
\caption{Percentage of choices \texttt{SS} made correctly.}
\label{tb:per_ss_made_correctly}
\end{table}

For each problem instance of the \texttt{IPC} 2011 benchmark we generate 12 heuristics: iPDB, \texttt{LM-Cut}, and 10 independently generated \texttt{GA-PDBs}, and we generate a data point ($\hat{J}(h_{1})/\hat{J}(h_{2})$ and $J(h_{1})/J(h_{2})$) for each possible pairwise combination of heuristics. In Table \ref{tb:per_ss_made_correctly} we present the percentage of data points falling in quadrants II and III. This percentage is above 50$\%$ for all domains, meaning that \texttt{SS} is able to correctly select the heuristic that yield smaller search trees in at least half of the binary decisions tested (i.e., should one choose $h_1$ or $h_2$?). 
%
 %the points represent good relation of heuristics. 
This experiment indicates that although \texttt{SS} produces inaccurate estimates of the A* search tree size, \texttt{SS} is capable of selecting the heuristic that will allow A* to generate fewer nodes in the majority of the cases tested. 

%\section{Are the GHS Selection Near Optimal?}
%
%In order to verify the practical effectiveness of \texttt{GHS} we have implemented the algorithm in Fast Downward (Helmert, \citeyear{helmert2006fast}) and tested the A$\sp{*}$ performance using subsets of heuristics selected by \texttt{GHS} while minimizing different objective functions.
%
%%We run two sets of experiments. 
%In the next experiment we verify whether the approximations $\hat{J}$ and $\hat{T}$ provided by \texttt{CS} and \texttt{SS} allow \texttt{GHS} selects good subset selections for guiding the A$\sp{*}$. %In the second set of experiments we test the effectiveness of \texttt{GHS} by measuring the total number of problem instances solved by A$\sp{*}$ using a heuristic subset selected by \texttt{GHS}.
%

% ---
\section{Empirical Evaluation of $\hat{J}$ and $\hat{T}$}



\noindent
In the previous experiment we tested whether \texttt{SS} allows one to correctly select the heuristic, from a set of size two, that yield the smallest A* search tree size. Next, we test whether the approximations of $\hat{J}$ provided by \texttt{CS} and \texttt{SS} allow \texttt{GHS} to make good subset selections from larger sets, with thousands of heuristics. This test is made by comparing $J(\zeta\sp{'})$ with $J(\zeta)$, which is provably minimal. We restrict our experiment to $J$ because, in contrast with $J$, there is no easy way to find the minimum of $T$ for a subset in general. 

We collect values of $J(\zeta)$ and $J(\zeta\sp{'})$ as follows. For each problem instance $\nabla$ in our test set we generate a set of \texttt{PDB} heuristics using the \texttt{GA-PDB} algorithm as described by Barley et al., (\citeyear{BarleySantiagoOver})---we call each \texttt{PDB} generated by this method a \texttt{GA-PDB}. The number of \texttt{GA-PDBs} generated is limited in this experiment by 1,200 seconds and 1GB of memory. Also, all \texttt{GA-PDBs} we generate have 2 millions entries each. The \texttt{GA-PDBs} generated form our $\zeta$ set. \texttt{GHS} then selects a subset $\zeta\sp{'}$ of $\zeta$. Finally, we use $h_{max}(\zeta\sp{'})$ and $h_{max}(\zeta)$ to independently try to solve $\nabla$. We call the system which uses A$\sp{*}$ with $h_{max}(\zeta)$ the \texttt{Max} approach. For \texttt{GHS} we allow 600 seconds for selecting $\zeta\sp{'}$ and for running A$\sp{*}$ with $h_{max}(\zeta\sp{'})$, and for \texttt{Max} we allow 600 seconds for running A$\sp{*}$ with $h_{max}(\zeta)$. Since we use 1,200 seconds to generate the heuristics, both \texttt{Max} and \texttt{GHS} are allowed 1,800 seconds in total for solving each problem. In this experiment we test both \texttt{CS} and \texttt{SS}.

\begin{table}[t]
\centering
%\setlength{\tabcolsep}{4.0 pt}
%\footnotesize
\caption{Ratios of the number of nodes generated using $h_{max}(\zeta')$ to the number of nodes generated using $h_{max}(\zeta)$.} 
\begin{tabular}{l r r r r r r }
\hline
 \multirow{2}{*}{Domain} & \multicolumn{2}{c}{\texttt{SS}}  & \multicolumn{2}{c}{\texttt{CS}} &  \multicolumn{1}{c}{\multirow{2}{*}{$|\zeta|$}}  &  \multicolumn{1}{c}{\multirow{2}{*}{$n$}} \\[\arrayrulewidth]
  \cline{2-5} 
 & \multicolumn{1}{c}{Ratio} & \multicolumn{1}{c}{$|\zeta'|$} & \multicolumn{1}{c}{Ratio} & \multicolumn{1}{c}{$|\zeta'|$} & & \\[\arrayrulewidth] 
\hline
Barman & 1.11 & 17.70 & 1.50 & 30.25 & 5,168.50 & 20 \\ 
Elevators & 11.50 & 2.00 & 1.04 & 21.00 & 168.00 & 1 \\
Floortile & 1.02 & 43.07 & 1.02 & 42.36 & 151.29 & 14 \\ 
Openstacks & 1.00 & 1.00 & 1.00 & 1.00 & 390.69 & 13 \\ 
Parking & 1.00 & 5.53 & 1.01 & 7.26 & 21.74 & 19 \\ 
Parcprinter & 3.62 & 1.00 & 2.21 & 13.00 & 1,189.00 & 1 \\ 
Pegsol & 1.00 & 31.00 & 1.00 & 57.00 & 90.00 & 2 \\ 
Scanalyzer & 1.23 & 30.57 & 1.57 & 19.43 & 72.86 & 7 \\ 
Sokoban & 1.32 & 7.00 & 1.01 & 24.00 & 341.00 & 1 \\
Tidybot & 1.00 & 2.35 & 1.00 & 8.59 & 3,400.18 & 17 \\ 
Transport & 1.00 & 14.70 & 1.03 & 14.30 & 171.70 & 10 \\ 
Visitall & 1.03 & 99.33 & 1.19 & 48.67 & 256.33 & 3 \\ 
Woodworking & 32.43 & 3.00 & 199.66 & 5.00 & 1,289.00 & 5 \\ 
\hline
\end{tabular}
\label{tb_one}
\end{table}

In this experiment we refer to the approach that runs A$\sp{*}$ guided by a heuristic subset selected by \texttt{GHS} using \texttt{CS} as \texttt{GHS+CS}. Similarly, we write \texttt{GHS+SS} when \texttt{SS} is used as predictor to make the heuristic subset selection.

Table~\ref{tb_one} shows the average ratios of $J(\zeta\sp{'})$ to $J(\zeta)$ for both \texttt{SS} and \texttt{CS} in different problem domains. The value of $J$, for a given problem instance, is computed as the number of nodes generated up to the largest $f$-layer which is fully expanded by all approaches tested (\texttt{Max}, \texttt{GHS+SS} and \texttt{GHS+CS}). We only present results for instances that are not solved during \texttt{GHS}'s \texttt{CS} sampling process. The column ``$n$'' shows the number of instances used to compute the averages of each row. We also show the average number of \texttt{GA-PDBs} generated (|$\zeta$|) and the average number of \texttt{GA-PDBs} selected by \texttt{GHS} (|$\zeta\sp{'}$|). This experiment shows that for most of the problems \texttt{GHS}, using \texttt{CS} or \texttt{SS}, is selecting good subsets of $\zeta$. For example, in Tidybot \texttt{GHS} selects only a few \texttt{GA-PDBs} out of thousands when using either \texttt{SS} or \texttt{CS}.


The exceptions in Table~\ref{tb_one} are the ratios for Elevators, Parcprinter, and Woodworking. In all three domains \texttt{GHS}+\texttt{SS} failed to make good heuristic selections because \texttt{SS} was not able to sample ``deep enough'' into the problem's search tree. For instance, for the Elevators instance \texttt{SS} was able to perform only 245 probes with the initial $f$-boundary within the time limit of 300 seconds. \texttt{GHS}+\texttt{CS} is able to make better selections for this instance because it is able to sample deeper into the search tree. For the Parcprinter instance as well as the Woodworking instances neither \texttt{SS} nor \texttt{CS} are able to sample deep enough to make good heuristic selections. These results suggest that \texttt{SS} could benefit from an adaptive approach for choosing \texttt{SS}'s number of probes. For example, for the Elevators instance \texttt{SS} could have performed better by sampling deeper into the tree with fewer probes. We intend to investigate this direction in future works.


\begin{table}[]
\centering
\caption{Coverage of \texttt{SS}, \texttt{CS} and \texttt{Max} on the 2011 IPC benchmarks. For GHS using only \texttt{GA-PDBs} heuristics.}
\label{my-label}
\begin{tabular}{lccc}
\hline
Domain      & SS & CS & Max \\ \hline
Barman      & 8          & 7          & 4           \\
Elevators   & 19         & 19         & 19          \\
Floortile   & 10         & 10         & 9           \\
Nomystery   & 20         & 20         & 20          \\
Openstacks  & 17         & 17         & 11          \\
Parcprinter & 17         & 15         & 14          \\
Parking     & 1          & 1          & 1           \\
Pegsol      & 19         & 19         & 19          \\
Scanalyzer  & 10         & 10         & 10          \\
Sokoban     & 20         & 20         & 20          \\
Tidybot     & 14         & 13         & 11          \\
Transport   & 14         & 14         & 14          \\
Visitall    & 18         & 18         & 18          \\
Woodworking & 12         & 11         & 12          \\ \hline
Total       & 199        & 194        & 182         \\ \hline
\end{tabular}
\label{tb_onlygapdbs}
\end{table}

%The \texttt{SS}'s ability of sampling deep into the search space is not always harmful. For example, \texttt{SS} allows \texttt{GHS} to make good selections for instances of the Woodworking domain. By contrast, \texttt{CS}'s systematic approach to sampling only allow a shallow sample of the A$\sp{*}$ search tree. As a result, \texttt{GHS} makes a limited selection of heuristics to guide A$\sp{*}$ search. While \texttt{GHS} using \texttt{SS} selects an average of 3 heuristics in Woodworking instances, \texttt{GHS} using \texttt{CS} selects only an average of 5 heuristics. This difference on sampling strategies reflects on the number of problems solved by A$\sp{*}$. While \texttt{GHS+SS} solves 12 instances of the Woodworking domain, \texttt{GHS+CS} solves only 11. 

In total, out of the 280 instances of the \texttt{IPC} 2011 benchmark set, \texttt{GHS+SS} solves 199 problem instances in this experiment, while \texttt{GHS+CS} only solves 194 problem instances. The numbers of instances solved are shown in Table \ref{tb_onlygapdbs}.

\section{Comparison with Other Planning Systems}

\noindent
The objective of the next experiment is to test the quality of the subset of heuristics \texttt{GHS} selects while optimizing different objective functions. Our evaluation metric is coverage, i.e., number of problems solved within a 1,800 second time limit. We note that the 1,800-second limit includes the time to generate $\zeta$, select $\zeta\sp{'}$, and run A$\sp{*}$ using $h_{max}(\zeta\sp{'})$. The $\zeta$ set of heuristics is composed of a number of different \texttt{GA-PDBs}, a \texttt{PDB} heuristic produced by the \texttt{iPDB} method and the \texttt{LM-Cut} heuristic. The generation of \texttt{GA-PDBs} is limited by 600 seconds and 1GB of memory. We use one fourth of 600 seconds to generate \texttt{GA-PDBs} with each of the following number of entries: $\{2 \cdot 10\sp{3}, 2 \cdot 10\sp{4}, 2 \cdot 10\sp{5}, 2 \cdot 10\sp{6}\}$. Our approach allows one to generate up to thousands of \texttt{GA-PDBs}. For every problem instance, we use exactly the same $\zeta$ set for \texttt{Max} and all \texttt{GHS} approaches.

\section{Systems Tested}

\noindent
\texttt{GHS} is tested while minimizing the A$\sp{*}$ search tree size (\texttt{Size}) and the A$\sp{*}$ running time (\texttt{Time}). We also use \texttt{GHS} to maximize the sum of heuristic values in the state space (\texttt{Sum}), as suggested by Rayner et al., (\citeyear{raynersss13}). Rayner et al., (\citeyear{raynersss13}) assumed that one could uniformly sample states in the state space in order to estimate the sum of the heuristic values for a given heuristic subset. Since we are not aware of any method to uniformly sample the state space of domain-independent problems, we adapted the Rayner et al.,'s method by using \texttt{SS} to estimate the sum of heuristic values in the search tree rooted at $\nabla$'s start state. We write \texttt{Size+SS} to refer to the approach that used A$\sp{*}$ guided by a heuristic selected by \texttt{GHS} while minimizing an estimate of the search tree size provided by \texttt{SS}. We follow the same pattern to name the other possible combinations of objective functions and prediction algorithms (e.g., \texttt{Time}+\texttt{CS}).

In addition to experimenting with all combinations of prediction algorithms (\texttt{CS} and \texttt{SS}) and objective functions (\texttt{Time}, \texttt{Size}), we also experiment with an approach that minimizes both the search tree size and the running time as follows. First we create a pool of heuristics $\zeta$ composed solely of \texttt{GA-PDB} heuristics, then we apply \texttt{GHS} while minimizing tree size and using \texttt{SS} as predictor. We call the selection of a subset of \texttt{GA-PDBs} as the \textit{first selection}. Once the first selection is made, we test all possible combinations of the resulting $h_{max}(\zeta\sp{'})$ added to \texttt{iPDB} and \texttt{LM-Cut} heuristics while minimizing the running time as estimated by \texttt{CS}---we call this step the \textit{second selection}. We call the overall approach \texttt{Hybrid}.
%As explained above, in this setting \texttt{GHS} minimizes $J$ and $T$ simultaneaously, as all heuristics in $\zeta$ have the same evaluation time (Theorem \ref{th:theorem_evaluation_time_heuristic})

%The intuition behind \texttt{Hibrid} is that we apply \texttt{GHS} with its strongest settings. \texttt{GHS} makes good selections respect to $J$ and $T$ when selecting from a pool of heuristics with the same evaluation time. After such a selection is made, we reduce the size of the pool of heuristics from possible thousands to only three (the maximum of a subset of the initial \texttt{GA-PDBs, iPDB}, and \texttt{LM-Cut}). With only three heuristics we are able to choose the exact combination that minimizes the A$\sp{*}$ running time the most. The reason we chose to use \texttt{SS} instead of \texttt{CS} for the first selection in \texttt{Hybrid} is that the former is able to make better subset selections in this setting, as suggested by the results discussed in the previous Chapter \ref{ch:rghs}. Finally, as we show below, \texttt{CS} is more effective if one is interested in minimizing the A$\sp{*}$ running time while selecting from a pool of heuristic with different evaluation times. That is why we use \texttt{CS} as predictor for the second selection in \texttt{Hybrid}.

We compare the coverage of the \texttt{GHS} approaches with several other state-of-the-art planners. Namely, we experiment with RIDA$\sp{*}$ \cite{BarleySantiagoOver}, two variants of StoneSoup (StSp1 and StSp2)~\cite{HelmertRK11}, two versions of Symba (SY1 and SY2) \cite{torralba2015phd}, and A$\sp{*}$ being independently guided by the maximum of all heuristics in $\zeta$ (\texttt{Max}), \texttt{iPDB}, \texttt{LM-cut} and \texttt{Merge $\&$ Shrink} (\texttt{M$\&$S}) \cite{nissim2011computing}. The results are \texttt{Hybrid}presented in Table~\ref{tb_two}.


\begin{table}[htb]
\tiny
\setlength{\tabcolsep}{1.8pt}
\centering
\caption{Coverage of different planning systems on the 2011 \texttt{IPC} benchmarks. For the \texttt{GHS} and \texttt{Max} approaches we also present the average number of heuristics \texttt{GHS} selects (|$\zeta\sp{'}$|).}
\begin{tabular}{lccccccccccccccc}
\hline
\multirow{2}{*}{Domains} & 
\multirow{2}{*}{\texttt{Hybrid}} & 
\multicolumn{2}{c}{\texttt{CS}} & 
\multicolumn{2}{c}{\texttt{SS}} & 
\multirow{2}{*}{\texttt{Sum}} & 
\multirow{2}{*}{\texttt{Max}} &
\multirow{2}{*}{RIDA$\sp{*}$} & 
\multirow{2}{*}{SY1} & 
\multirow{2}{*}{SY2} & 
\multirow{2}{*}{StSp1} & 
\multirow{2}{*}{StSp2} & 
\multirow{2}{*}{iPDB} & 
\multirow{2}{*}{LM-Cut} & 
\multirow{2}{*}{M$\&$S} \\ \cline{3-6}
                         &                                    & \texttt{Time} & \texttt{Size} & \texttt{Time} & \texttt{Size} &                                 &                       &                      &                      &                        &                        &                                 &                       &                         &                         \\ \hline
Barman&         7&      5&     4&    4&    4&    4&   4&    4&    10&  11&    4&    4&    4&     4&   4\\
Elevators&     19&     19&    19&   19&   19&   19&  19&   19&    20&  20&   18&   18&   18&    17&  12\\
Floortile&     15&     14&    14&   14&   14&   14&  14&   14&    14&  14&   14&   14&   14&     8&  10\\
Nomystery&     20&     20&    20&   19&   19&   20&  20&   20&    16&  16&   20&   20&   14&    19&  18\\
Openstacks&    17&     17&    15&   17&   15&   15&  11&   15&    20&  20&   17&   17&   15&    17&  17\\
Parcprinter&   18&     18&    18&   16&   15&   19&  18&   18&    17&  17&   18&   18&   17&    16&  16\\
Parking&        7&      7&     2&    7&    2&    2&   2&    7&     2&   1&    5&    5&    2&     7&   7\\
Pegsol&        18&     18&    19&   19&   19&   19&  19&   19&    19&  20&   19&   19&   17&    20&  19\\
Scanalyzer&    13&     14&    12&   11&   14&   14&  14&   14&     9&   9&   14&   14&   12&    10&  11\\
Sokoban&       20&     20&    20&   20&   20&   20&  20&   20&    20&  20&   20&   20&   20&    20&  20\\
Tidybot&       17&     16&    16&   16&   16&   16&  15&   17&    15&  17&   16&   16&   16&    14&   9\\
Transport&     14&     13&    10&   11&   13&   11&   9&   10&    10&  11&    7&    8&    6&     8&   7\\
Visitall&      18&     18&    18&   15&   18&   18&  18&   18&    12&  12&   16&   16&   10&    16&  16\\
Woodworking&   16&     15&    15&   12&   16&   16&  16&   15&    20&  20&   15&   15&   15&     9&   9\\ \hline
Total&        219&    214&   202&  200&  204&  207& 199&  210&   204& 208&  203&  204&  180&   185& 175\\ \hline
\end{tabular}
\label{tb_two}
\end{table}

\section{Discussion of the Results}
\noindent
The system that solves the largest number of instances is \texttt{Hybrid} (219 in total). Its combination of the strengths of both \texttt{SS} and \texttt{CS} has proven particularly effective on the Barman domain where \texttt{Hybrid}'s first selection contains good subsets of GA-PDBs and its second selection recognizes when it must not add the iPDB and LMCut heuristics to the first selection. As a result, \texttt{Hybrid} solves more problems on this domain than any other \texttt{GHS} approach. 

\texttt{Time}+\texttt{CS} also performs well---it solves 214 problems.  \texttt{Hybrid} and \texttt{Time}+\texttt{CS} are superior to all other approaches tested. %For example, 
%\texttt{Size}+\texttt{SS} and \texttt{Sum} solve only 204 and 203 problems, respectively. 
When minimizing $\hat{J}$ or maximizing \texttt{Sum}, \texttt{GHS} tends to add accurate heuristics to the selected subset, irrespective of their evaluation time. Thus, \texttt{GHS} frequently selects LMCut which is often the heuristic that most reduces the search tree size and most increases the sum of heuristic values. However, LMCut is computationally expensive, and often  the search is faster if LMCut is not in $\zeta'$. Both \texttt{Hybrid} and \texttt{Time}+\texttt{CS} often recognize when LMCut should not be in $\zeta'$ because they account for the heuristics' evaluation time.
 %Our results are in accordance with those of Barley et al.~\shortcite{barleyFR2014}, as they suggested that one should optimize the search running time while selecting a heuristic subset. Here we test different objective functions with different predictions algorithms and the two best results are achieved when one minimizes the A* running time. 

Interestingly, while \texttt{Time}+\texttt{CS} solves 214 instances, \texttt{Time}+\texttt{SS} solves only 200. We conjecture that this is due to \texttt{SS} not detecting duplicate nodes during sampling and thus substantially overestimating A*'s running time. As a result, similarly to the \texttt{Size} and \texttt{Sum} approaches, \texttt{Time}+\texttt{SS} often mistakenly adds the accurate but expensive LMCut heuristic in cases where A*  would be faster without LMCut. %For example, although iPDB tends to prune fewer nodes than LMCut in Parking instances, its time efficiency makes it the heuristic of choice in that domain. \texttt{Time}+\texttt{CS} solves 7 Parking instances as it correctly selects iPDB and leaves LMCut out of $\zeta'$. By contrast, likely due to its prediction overestimation, \texttt{Time}+\texttt{SS} wrongly estimates that LMCut will reduce the overall search time and selects that heuristic. Note that \texttt{Size}+\texttt{CS} and \texttt{Size}+\texttt{SS} also do poorly in Parking instances as they also always select LMCut. 

RIDA* is the most similar system to \texttt{GHS}; it selects a subset of heuristics by using an evaluation method similar to \texttt{CS}. %RIDA* starts with an empty subset and evaluates all subsets of size $i$ before evaluating subsets of size $i+1$. 
Starting with an empty subset, it evaluates all subsets of size $i$ before evaluating subsets of size $i+1$. This limits RIDA* to considering only tens of heuristics in its pool. Specifically, RIDA* uses 42 GA-PDBs, iPDB, and LMCut in its pool. By contrast, \texttt{GHS} may consider thousands of heuristics. % while making its selection. %For example, when using $time$ of 1,200 seconds \texttt{GHS} selects from a pool of 6,182.15 heuristics on average. In terms of coverage, RIDA* with its best parameters solves 187 problems, while \texttt{GHS} with its best parameters is able to solve 191 problems.
%

Selecting from large sets of heuristics can be helpful, %if used with methods such as the GA-PDBs, which are able to quickly generate a large set of heuristics, 
even if most of the heuristics in the set are redundant with each other---as is the case with the GA-PDBs. The process of generating GA-PDBs is stochastic, thus one increases the chances of generating a helpful heuristic by generating a large number of them. \texttt{GHS} is an effective method for selecting a small set of informative heuristics from a large set of mostly uninformative ones. This is illustrated in Table~\ref{tb_two} on the Transport domain. Compared to systems which use multiple heuristics (StSp 1 and 2, and RIDA*), \texttt{Hybrid} solves the largest number of Transport instances, which is due to the selection of a few key GA-PDBs.

The best \texttt{GHS} approach, \texttt{Hybrid}, substantially outperforms  \texttt{Max}; \texttt{Hybrid} solves 20 more instances than \texttt{Max}. Finally, \texttt{Hybrid} and \texttt{Time}+\texttt{CS} substantially outperform all other approaches tested, with RIDA* being the closest competitor with 210 instances solved. 


